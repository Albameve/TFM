# -*- coding: utf-8 -*-
"""TFM Modelos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NellzCLF89PQ9dwNVqvPKk6wBa5DwsSZ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#for text pre-processing
import re, string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import genesis
nltk.download('genesis')
nltk.download('wordnet')
genesis_ic = wn.ic(genesis, False, 0.0)

#for model-building
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error, r2_score
from sklearn import model_selection, naive_bayes, svm
# bag of words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
#for word embedding
import gensim
from gensim.models import Word2Vec
from nltk import pos_tag
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
import os
import sys
import numpy
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from sklearn.datasets import load_files
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier

import pandas as pd
import os
from sklearn.preprocessing import LabelEncoder

from nltk.stem.porter import PorterStemmer
from nltk.stem.lancaster import LancasterStemmer
from tensorflow.keras import Model
from sklearn import linear_model

"""Vamos a usar la base de datos que se encuentra en Google Drive por ello usamos el siguiente comando: 

"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Nuestra base de datos se va a llamar df_train"""

df_train=pd.read_csv('/content/gdrive/My Drive/PAEV1 - Final.csv')

df_test=pd.read_csv('/content/gdrive/My Drive/PAEV1 - Problemas.csv')

"""Análisis exploratorio de los datos """

df_train.describe()

x=df_train['Clase'].value_counts()
print(x)
sns.barplot(x.index,x)

df_train['word_count'] = df_train['Problema'].apply(lambda x: len(str(x).split()))
print(df_train[df_train['Clase']=="Igualación"]['word_count'].mean()) #Problemas de igualación
print(df_train[df_train['Clase']=="Combinación"]['word_count'].mean()) #Problemas de combinación
print(df_train[df_train['Clase']=="Cambio"]['word_count'].mean()) #Problemas de igualación
print(df_train[df_train['Clase']=="Comparación"]['word_count'].mean()) #Problemas de comparación

# CONTEO DE PALABRAS
fig,(ax1,ax2,ax3,ax4) = plt.subplots(1,4)
train_words=df_train[df_train['Clase']=="Igualación"]['word_count']
ax1.hist(train_words,color='red')
ax1.set_title('Igualación')
train_words=df_train[df_train['Clase']=="Combinación"]['word_count']
ax2.hist(train_words,color='green')
ax2.set_title('Combinación')
train_words=df_train[df_train['Clase']=="Cambio"]['word_count']
ax3.hist(train_words,color='blue')
ax3.set_title('Cambio')
train_words=df_train[df_train['Clase']=="Comparación"]['word_count']
ax4.hist(train_words,color='pink')
ax4.set_title('Comparación')
fig.suptitle('PAEV')
plt.show()

# NÚMERO DE LETRAS
df_train['char_count'] = df_train['Problema'].apply(lambda x: len(str(x)))
print(df_train[df_train['Clase']=="Combinación"]['char_count'].mean())
print(df_train[df_train['Clase']=="Igualación"]['char_count'].mean()) 
print(df_train[df_train['Clase']=="Cambio"]['char_count'].mean())
print(df_train[df_train['Clase']=="Combinación"]['char_count'].mean())

df_train.isnull()

"""Realizamos el procesamiento de texto 

"""

#convertir a minúsculas, quitar y eliminar puntuaciones
def preprocess(text):
    text = text.lower() 
    text=text.strip()  
    text=re.compile('<.*?>').sub('', text) 
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  
    text = re.sub('\s+', ' ', text)  
    text = re.sub(r'\[[0-9]*\]',' ',text) 
    text=re.sub(r'[^\w\s]', '', str(text).lower().strip())
    text = re.sub(r'\d',' ',text) 
    text = re.sub(r'\s+',' ',text) 
    return text

 
# ELIMINACIÓN DE PALABRAS DE PARO
def stopword(string):
    a= [i for i in string.split() if i not in stopwords.words('english')]
    return ' '.join(a)
#LEMMATIZATION
# Inicializar el lemmatizer(
wl = WordNetLemmatizer()

# Esta es una función auxiliar para mapear etiquetas de posición NTLK
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
# Tokenizar la oración
def lemmatizer(string):
    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Coge la posición 
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token
    return " ".join(a)

def finalpreprocess(string):
    return lemmatizer(stopword(preprocess(string)))

df_train['clean_text'] = df_train['Problema'].apply(lambda x: finalpreprocess(x))

"""Mostramos como que queda nuestra variable 'Clean_text' que sera la que este "limpia""""

df_train['clean_text']

"""Una vez que tenemos esto, dividimos el dataset en train y test """

#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST
X_train, X_test, y_train, y_test = train_test_split(df_train["clean_text"],df_train["Clase"],test_size=0.2,shuffle=True)
#Word2Vec
# Word2Vec runs on tokenized sentences
X_train_tok= [nltk.word_tokenize(i) for i in X_train]  
X_test_tok= [nltk.word_tokenize(i) for i in X_test]

#Tf-Idf
tfidf_vectorizer = TfidfVectorizer(use_idf=True)
X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) 
X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)
#building Word2Vec model
class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        #self.dim = len(word2vec.itervalues().next())
        self.dim = 100

    def fit(self, X, y):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']]   
model = Word2Vec(df_train['clean_text_tok'],min_count=1)
w2v=dict(zip(model.wv.index2word,model.wv.vectors))
modelw=MeanEmbeddingVectorizer(w2v)

# converting text to numerical data using Word2Vec
X_train_vectors_w2v=modelw.transform(X_train_tok)
X_test_vectors_w2v = modelw.transform(X_test_tok)

"""Regresión lineal

Transformanos las variables cateogoricas a numéricas para poner usarlas en el modelo lineal
"""

from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

values = array(y_train)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(values)
print(y_train_encoded)

values2 = array(y_test)
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(values2)
print(y_test_encoded)

#AJUSTE DEL MODELO DE CLASIFICACIÓN mediante Regresión Lineal tf-idf)
regr = linear_model.LinearRegression()
regr.fit(X_train_vectors_tfidf, y_train_encoded)  
#Predecir el valor y para el conjunto de datos de prueba
y_predict = regr.predict(X_test_vectors_tfidf)
# Los coeficientes 
print("Coeficientes: \n", regr.coef_)
# El error cuadrático medio
print(" El error cuadrático medio: %.2f" % mean_squared_error(y_test_encoded, y_predict))
# El coeficiente de determinación: 1 es predicción perfecta
print("Coeficiente de determinación: %.2f" % r2_score(y_test_encoded, y_predict))

"""Regresión Logística"""

#FITTING THE CLASSIFICATION MODEL using Logic Regression(tf-idf)
lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')
lr_tfidf.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
y_predict = lr_tfidf.predict(X_test_vectors_tfidf)
y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,y_predict))
matriz_confusion= confusion_matrix(y_test, y_predict)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("LogisticRegression Accuracy Score->",accuracy_score(y_predict,y_test)*100)

#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)
lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')
lr_w2v.fit(X_train_vectors_w2v, y_train)  #model
#Predecir el valor y para el conjunto de datos de prueba
y_predict = lr_w2v.predict(X_test_vectors_w2v)
y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]
print(classification_report(y_test,y_predict))
matriz_confusion= confusion_matrix(y_test, y_predict)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("Logistic Regression Accuracy Score->",accuracy_score(y_predict,y_test)*100)

"""Naive Bayes"""

#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)
nb_tfidf = MultinomialNB()
nb_tfidf.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
y_predict = nb_tfidf.predict(X_test_vectors_tfidf)
y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,y_predict))
matriz_confusion= confusion_matrix(y_test, y_predict)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("NB Accuracy Score->",accuracy_score(y_predict,y_test)*100)

"""SVM """

#FITTING THE CLASSIFICATION MODEL Classifier - Algorithm - SVM
SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)
SVM.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
predictions_SVM = SVM.predict(X_test_vectors_tfidf)
y_prob = SVM.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,predictions_SVM))
matriz_confusion= confusion_matrix(y_test, predictions_SVM)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("SVM Accuracy Score->",accuracy_score(predictions_SVM,y_test)*100)

"""XGB"""

#FITTING THE CLASSIFICATION MODEL Classifier - XGB
XGBC = XGBClassifier()
XGBC .fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
predictions_XGBC = XGBC.predict(X_test_vectors_tfidf)
y_prob = XGBC.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,predictions_XGBC))
matriz_confusion= confusion_matrix(y_test, predictions_XGBC)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("XGBC  Accuracy Score->",accuracy_score(predictions_XGBC,y_test)*100)

"""**Vecinos Cercanos **

KN3
"""

#FITTING THE CLASSIFICATION MODEL Classifier - KNeighbors3
KNeighbors = KNeighborsClassifier(n_neighbors=3)
KNeighbors.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
predictions_KNeighbors = KNeighbors.predict(X_test_vectors_tfidf)
y_prob = KNeighbors.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,predictions_KNeighbors))
matriz_confusion= confusion_matrix(y_test, predictions_KNeighbors)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("KNeighbors  Accuracy Score->",accuracy_score(predictions_KNeighbors,y_test)*100)

"""KN5"""

#FITTING THE CLASSIFICATION MODEL Classifier - KNeighbors5
KNeighbors = KNeighborsClassifier(n_neighbors=5)
KNeighbors.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
predictions_KNeighbors = KNeighbors.predict(X_test_vectors_tfidf)
y_prob = KNeighbors.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,predictions_KNeighbors))
matriz_confusion= confusion_matrix(y_test, predictions_KNeighbors)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("KNeighbors  Accuracy Score->",accuracy_score(predictions_KNeighbors,y_test)*100)

"""KN7"""

#FITTING THE CLASSIFICATION MODEL Classifier - KNeighbors7
KNeighbors = KNeighborsClassifier(n_neighbors=7)
KNeighbors.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
predictions_KNeighbors = KNeighbors.predict(X_test_vectors_tfidf)
y_prob = KNeighbors.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,predictions_KNeighbors))
matriz_confusion= confusion_matrix(y_test, predictions_KNeighbors)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("KNeighbors  Accuracy Score->",accuracy_score(predictions_KNeighbors,y_test)*100)

"""Radom Forest"""

#FITTING THE CLASSIFICATION MODEL Classifier - Random Forest 
RandomForest = RandomForestClassifier()
RandomForest.fit(X_train_vectors_tfidf, y_train)  
#Predecir el valor y para el conjunto de datos de prueba
predictions_RandomForest = RandomForest.predict(X_test_vectors_tfidf)
y_prob = RandomForest.predict_proba(X_test_vectors_tfidf)[:,1]
print(classification_report(y_test,predictions_RandomForest))
matriz_confusion= confusion_matrix(y_test, predictions_RandomForest)
sns.heatmap(matriz_confusion, annot=True, cmap="Oranges")
print("RandomForest Accuracy Score->",accuracy_score(predictions_RandomForest,y_test)*100)

#Preprocesamiento del nuevo conjunto de datos
df_test['clean_text'] = df_test['Problema'].apply(lambda x: finalpreprocess(x)) #preprocesamiento de los datos 
X_test=df_test['clean_text'] 
#convirtiendo palabras a datos numéricos usando tf-idf
X_vector=tfidf_vectorizer.transform(X_test)
#use el mejor modelo para predecir el valor 'objetivo' para el nuevo conjunto de datos 
y_predict = RandomForest.predict(X_vector)      
y_prob = RandomForest.predict_proba(X_vector)[:,1]
df_test['predict_prob']= y_prob
df_test['target']= y_predict
final=df_test[['clean_text','target']].reset_index(drop=True)
print(final)